<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.7.0/font/bootstrap-icons.css">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">

  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js" integrity="sha384-7+zCNj/IqJ95wo16oMtfsKbZ9ccEh31eOz1HGyDuCQ6wgnyJNSYdrPa03rtR1zdB" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js" integrity="sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13" crossorigin="anonymous"></script>
  <meta name="keywords" content="ENLSP2023, ENLSP III, enlsp iii, NLP, KD, knowledge distillation, workshop,natural language processing, deep learning, machine learning, neurips workshop, neurips workshop 2023, model compression, Efficient Natural Language and Speech Processing,tinyML,BERT,tinyBERT,roberta,distilRoberta,distilBERT,knowledge transfer,efficient knowledge transfer,speech,pre-trained language models,language models,GPT,GPT compression,optimization,speech processing,speech optimization,Multi-domain training,fast pre-training,multimodal,Efficient Training,Data Efficiency,Edge Intelligence,zero-shot learning,few-shot learning,data augumentation,NEURIPS,neurips 2023, NIPS">
  <meta name="google-site-verification" content="2FYQh4GH2FAIelCcPu2MkMhMqFo76u69G5uxcopfuC8" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ENLSP NeurIPS Workshop 2024 | ENLSP highlights some fundamental problems in NLP and speech processing related to efficiency of the models, training and inference for the general ML and DL communities.</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="ENLSP NeurIPS Workshop 2024" />
<meta name="author" content="ENLSP NeurIPS Workshop 2024" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="ENLSP highlights some fundamental problems in NLP and speech processing related to efficiency of the models, training and inference for the general ML and DL communities." />
<meta property="og:description" content="ENLSP highlights some fundamental problems in NLP and speech processing related to efficiency of the models, training and inference for the general ML and DL communities." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="ENLSP NeurIPS Workshop 2024" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ENLSP NeurIPS Workshop 2024" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"ENLSP NeurIPS Workshop 2024"},"description":"ENLSP highlights some fundamental problems in NLP and speech processing related to efficiency of the models, training and inference for the general ML and DL communities.","headline":"ENLSP NeurIPS Workshop 2024","name":"ENLSP NeurIPS Workshop 2024","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="ENLSP NeurIPS Workshop 2024" />
<link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200&display=swap" rel="stylesheet">
</head>
<body>
<nav class="navbar navbar-expand-lg navbar-light bg-light">
  <div class="container-fluid">
    <a class="navbar-brand" href="#">ENLSP 2024</a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
		
        
            
            
            
				<li class="nav-item">
				  <a class="nav-link text-nowrap " aria-current="page" href="/index.html">Home</a>
				</li>
            
        
            
            
            
				<li class="nav-item">
				  <a class="nav-link text-nowrap " aria-current="page" href="/index.html#call_for_papers">Call for papers</a>
				</li>
            
        
            
            
            
				<li class="nav-item">
				  <a class="nav-link text-nowrap " aria-current="page" href="/index.html#organizers">Organizers</a>
				</li>
            
        
            
            
            
				<li class="nav-item">
				  <a class="nav-link text-nowrap " aria-current="page" href="/index.html#speakers">Speakers</a>
				</li>
            
        
            
            
            
				<li class="nav-item">
				  <a class="nav-link text-nowrap " aria-current="page" href="/index.html#schedule">Schedule</a>
				</li>
            
        
            
            
            
				<li class="nav-item">
				  <a class="nav-link text-nowrap " aria-current="page" href="/index.html#technical_committee">Technical committee</a>
				</li>
            
        
            
            
            
				<li class="nav-item">
				  <a class="nav-link text-nowrap " aria-current="page" href="/accepted_papers.html">Accepted papers</a>
				</li>
            
        
            
            
            
				<li class="nav-item">
				  <a class="nav-link text-nowrap " aria-current="page" href="https://neurips2023-enlsp.github.io/">ENLSP2023</a>
				</li>
            
         
      </ul>
    </div>
  </div>
</nav>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><p><img src="/images/banner2024.png" style="pointer-events: none; user-select: none;" /></p>
<p>
The fourth version of the Efficient Natural Language and Speech Processing (ENLSP-IV) workshop will focus on the future of large language models and their emerging applications on different domains such as natural language, speech processing, and biological sequences; and the target is on how to make them more efficient in terms of <b>Data</b>, <b>Model</b>, <b>Training</b>, and <b>Inference</b> for real-world applications as well as academic research.  The workshop program offers an interactive platform for gathering different experts and talents from academia and industry through invited talks, panel discussion, paper submissions, reviews, interactive posters, oral presentations and a mentorship program.
This will be a unique opportunity to discuss and share challenging problems, build connections,  exchange ideas and brainstorm solutions, and foster future collaborations. The topics of this workshop can be of interest for people working on general machine learning, deep learning, optimization, theory and NLP &amp; Speech applications. 
</p>

<h2 class="blackpar_title" id="overview">Overview</h2>
<p>
With the emergence of large language and speech models (such as GPT-3, GPT-4, wav2vec, Hubert, wavLM, Whisper, PALM, LLaMA, and PALM 2) and then their variants which are fine-tuned on following instructions (such as Instruct-GPT, Alpaca, Dolly) and especially, conversational language models (such as ChatGPT, Bard, Vicuna, StableLM-Tuned, OpenAssistant), we have taken one significant step towards mimicking the human intelligence by machines. The contributions of Language models did not even stop at this level, their emerging usage in biological (protein language models such as protTrans models, ESM, protGPT2) and chemical domains (small molecule and polymer language models like SMILESBERT, polyBERT, and transPolymers) have further expanded their applications across various scientific disciplines. As a result, these models are revolutionizing the way we approach research and knowledge discovery, paving the way for more innovative breakthroughs in diverse fields.

This great success has come with the price of pre-training these large models on a huge amount of data, fine-tuning them with instruction-based data and human supervised fine-tuning data, and extensive engineering efforts. Despite the great success of these large models, it is evident that most of them are largely over-parameterized and their efficiency is under question. Lack of efficiency can largely limit the application of these advanced techniques in practice. Training, adapting or deploying these large models on devices or even cloud services with limited memory and computational power can be very challenging.
<br /><br />
While these current achievements have paved the road for the faster progress of improving large foundation models from different aspects in the future, we need to address different efficiency issues of these models at the same time. For example, in natural language, it has been shown that larger model sizes reveal more zero-shot or few-shot (in-context learning) capabilities in handling different tasks. However, collecting data, pre-training and maintaining such large models can be very expensive. In terms of training data, it is still not very clear to what extent expanding the training data can improve these pre-trained models and whether we can compress pre-training data without much sacrificing the performance. The problem of efficiency becomes more critical when we think about pre-training multimodal models.  At the time of deployment, designing proper prompts for different tasks can be very arbitrary and time consuming. Fine-tuning large language models with billions of parameters is very costly. One can think of transferring the knowledge of large foundation models to smaller models (by distillation or symbolic distillation) but still we do not have a straightforward recipe for this task. Furthermore, there is a debate in the literature that to what extent we can transfer the knowledge of powerful large black-box models such as ChatGPT to smaller models in specific or general domains. Additionally, due to the huge size of the foundation models, applying model compression techniques to them is not an easy task. 

In the light of advances in large protein language models and their application in biology, this year there will be a special track focused on emerging protein language models, their pretraining, fine tuning, applications, and approaches for improving their efficiency. 
</p>

<!-- Call for Papers -->
<h2 class="blackpar_title" id="call_for_papers">Call for Papers</h2>
<p>
It is of vital importance to invest on future of large foundation models by enhancing their efficiency in terms of data, modeling, training and inference from different perspectives highlighted in the workshop.  In this regard, we share some active research topics in this domain which might be of interest to the NeurIPS community to get their participation, ideas and contributions.  The scope of this workshop includes, but not limited to, the following topics:
<br /><br />
<b>Efficient Pre-Training</b> How can we reduce the cost of pre-training new models?
<ul>
	<li>Accelerating the pre-training process</li>
	<li>Efficient initialization and hyper-parameter tuning (HPT)</li>
	<li>Data vs. scale of pre-trained models</li>
	<li>Efficient Multimodal (e.g., text–speech) pre-trained models and efficiency issues related to it</li>
	<li>New efficient architectures (e.g. using sparse structures or mixture of experts (MoEs)) or new training objectives for pre-trained models</li>
</ul>
<b>Efficient Fine-tuning</b> Fine-tuning the entire parameters of large pre-trained models on downstream
tasks can be expensive and it is prone to overfitting.
<ul>	
	<li>Efficient prompt engineering and in-context learning</li>
	<li>Parameter-efficient tuning solutions (i.e. training only a portion of the entire network)</li>
	<li>Accelerating the fine-tuning process (e.g. by improving the optimizer, and layer-skipping)</li>
</ul>
<b>Data Efficiency</b> Pre-training (with unlabeled data) and fine-tuning (with labeled data) are both data hungry processes. Labeling data and incorporating human annotated data or human feedback are very time consuming and costly. Here we would like to address "how to reduced the costs borne by data?"
<ul>
	<li>Sample efficient training, training with less data, few-shot and zero-shot learning</li>
	<li>How to reduce the requirements for human labeled data?</li>
	<li>Can we rely on machine generated data for training models? (e.g. data collected from ChatGPT)</li>
	<li>Data compression, data distillation</li>
</ul>
<b>Efficient Deployment</b> How can we reduce the inference time or memory footprint of a trained model for a particular task?
<ul>
	<li>Relying on in-context learning and prompt engineering of large language models or fine-tuning smaller models (by knowledge transfer from larger models)?</li>
	<li>Neural model compression techniques such as (post-training) quantization, pruning, layer decom- position and knowledge distillation (KD) for NLP and Speech</li>
	<li>Impact of different efficient deployment solutions on the inductive biases learned by the original models (such as OOD generalization, in-context learning, in-domain performance, hallucination).</li>
</ul>
<b>Special track: Protein Language Models </b> Emergence and the future of language models for biological sequences and how to make them more efficient.
<ul>
	<li>Protein language models and their applications</li>
	<li>Refining the pretraining algorithm and/or model architecture of LLMs to optimize performance in the protein domain.</li>
	<li>Optimizing the curriculum learning (order of pretraining data presentation) for more efficient pre-training or fine tuning of protein language models</li>
	<li>Efficient remote homology via dense retrieval using protein language models</li>
	<li>Combining sequence and 3D structure in pretraining or fine-tuning of the models</li>
	<li>Multi-modal language models for biological sequences.</li>
</ul>
<b>Other Efficient Applications</b>
<ul>
	<li>Knowledge localization, knowledge editing, or targeted editing/training of foundation models</li>
	<li>Efficient dense retrieval and search</li>
	<li>Efficient graphs for NLP</li>
	<li>Training models on device</li>
	<li>Incorporating external knowledge into pre-trained models</li>
	<li>Efficient Federated learning for NLP: reduce the communication costs, tackling heterogeneous data, heterogeneous models.</li>
</ul>

</p>

<h2 class="blackpar_title">Submission Instructions</h2>
<p>
You are invited to submit your papers in our CMT submission portal <a href="https://cmt3.research.microsoft.com/ENLSP2024">(Link)</a>. All the submitted papers have to be anonymous for double-blind review. We expect each paper will be reviewed by at least three reviewers. The content of the paper (excluding the references and supplementary materials) should not be longer than 4 pages, strictly following the NeurIPS template style. 
<br /><br />
Authors can submit up to 100 MB of supplementary materials separately. Authors are highly encouraged to submit their codes for reproducibility purposes. According to the guideline of the NeurIPS workshops, already published papers are not encouraged for submission, but you are allowed to submit your ArXiv papers or the ones which are under submission. Moreover, a work that is presented at the main NeurIPS conference should not appear in a workshop. Please make sure to indicate the complete list of conflict of interests for all the authors of your paper. To encourage higher quality submissions, our sponsors are offering the <b>Best Paper</b> and the <b>Best Poster</b> Award to qualified outstanding original oral and poster presentations (upon nomination of the reviewers). Also, we will give one outstanding paper certification for our special track of protein language models. Bear in mind that our workshop is not archival, but the accepted papers will be hosted on the workshop website.
</p>

<h2 class="blackpar_title">Important Dates:</h2>
<p>
<ul>
	<li>Submission Deadline: August 30, 2024 <b>AOE</b> </li>
	<li>Acceptance Notification: September 29, 2024 <b>AOE</b> </li>
	<li>Camera-Ready Submission: October 11, 2024 <b>AOE</b> </li>
	<li>Workshop Date: <b>TBD </b></li>
</ul>
</p>

<!--Confirmed Speakers-->
<h2 class="blackpar_title" id="speakers">Confirmed Keynote Speakers</h2>
<p>


	
		
		<div class="row_perso">
		
		
			
				<div class="card_perso column_perso">
				  <img src="/images/danqi_2019.jpg" alt="Danqi Chen" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h5>
						<b>Danqi Chen</b>
					</h5>
					<h6>
						Princeton
					</h6>
					</center>
				  </div>
				</div>
			
				<div class="card_perso column_perso">
				  <img src="/images/christopher_re.jpg" alt="Christopher Re" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h5>
						<b>Christopher Re</b>
					</h5>
					<h6>
						Stanford
					</h6>
					</center>
				  </div>
				</div>
			
				<div class="card_perso column_perso">
				  <img src="/images/Weizhu_Chen.jpg" alt="Weizhu Chen" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h5>
						<b>Weizhu Chen</b>
					</h5>
					<h6>
						Microsoft
					</h6>
					</center>
				  </div>
				</div>
			
				<div class="card_perso column_perso">
				  <img src="/images/peter_clark.jpg" alt="Peter Clark" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h5>
						<b>Peter Clark</b>
					</h5>
					<h6>
						Allen Institute for AI
					</h6>
					</center>
				  </div>
				</div>
			
		
		</div>
	
		
		<div class="row_perso">
		
		
			
				
				<div class="card_perso column_perso" style="margin-left:13%;">
				  <img src="/images/Hananeh_Hajishirzi.jpg" alt="Hananeh Hajishirzi" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h5>
						<b>Hananeh Hajishirzi</b>
					</h5>
					<h6>
						University of Washington
					</h6>
					</center>
				  </div>
				</div>
				
			
				
				<div class="card_perso column_perso">
				  <img src="/images/navdeep-jaitly.jpg" alt="Navdeep Jaitly" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h5>
						<b>Navdeep Jaitly</b>
					</h5>
					<h6>
						Apple
					</h6>
					</center>
				  </div>
				</div>
				
			
				
				<div class="card_perso column_perso">
				  <img src="/images/Maciej-Besta.png" alt="Maciej Besta" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h5>
						<b>Maciej Besta</b>
					</h5>
					<h6>
						ETH Zurich
					</h6>
					</center>
				  </div>
				</div>
				
			
		
		</div>
	

</p>

<h2 class="blackpar_title" id="speakers">Panelists</h2>
<p>


	
		
		<div class="row_perso">
		
		
			
				
				<div class="card_perso column_perso" style="margin-left:13%;">
				  <img src="/images/MarjanGhazvininejad.jpg" alt="Marjan Ghazvini Nejad" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h6>
						<b>Marjan Ghazvini Nejad</b>
						<br />
						Meta
					</h6>
					</center>
				  </div>
				</div>
				
			
				
				<div class="card_perso column_perso">
				  <img src="/images/luhou.jpg" alt="Lu Hou" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h6>
						<b>Lu Hou</b>
						<br />
						Huawei
					</h6>
					</center>
				  </div>
				</div>
				
			
				
				<div class="card_perso column_perso">
				  <img src="/images/joelhestness.jpg" alt="Joel Hestness" class="img_card_perso" />
				  <div class="container_perso">
					<center>
					<h6>
						<b>Joel Hestness</b>
						<br />
						Cerebras
					</h6>
					</center>
				  </div>
				</div>
				
			
		
		</div>
	


</p>

<!-- Schedule -->
<h2 class="blackpar_title" id="schedule">Schedule</h2>
<p>



	
		
	
		
			<div class="modal fade" id="talk_2" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-body">
							<b>Title:</b> (<b>KeyNote Talk</b>) Title TBD
							<br />
							<b>Presenter:</b> Prof. Maciej Besta
							<br />
							
								<b>Bio</b><p class="par_panel_perso"><b> Maciej Besta</b> leads research on efficient computations for large language models and graphs at ETH Zurich, with a background in high-performance computing and a passion for irregular data structures. </p>
							
							<b>Abstract</b><p class="par_panel_perso">Abstract TBD </p>
						</div>
						<div class="modal-footer">
							<button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
						</div>
					</div>
				</div>
			</div>
		
	
		
			<div class="modal fade" id="talk_3" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-body">
							<b>Title:</b> (<b>KeyNote Talk</b>) Title TBD
							<br />
							<b>Presenter:</b> Dr. Peter Clark
							<br />
							
								<b>Bio</b><p class="par_panel_perso"><b>Peter Clark</b> is a leading AI researcher at the Allen Institute for AI, he currently directs a project to develop AI agents capable of scientific discovery. </p>
							
							<b>Abstract</b><p class="par_panel_perso">Abstract TBD </p>
						</div>
						<div class="modal-footer">
							<button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
						</div>
					</div>
				</div>
			</div>
		
	
		
			<div class="modal fade" id="talk_4" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-body">
							<b>Title:</b> Accepted Oral Presentations (4)
							<br />
							<b>Presenter:</b> TBD
							<br />
							
								<b>Authors</b><p class="par_panel_perso">TBD </p>
							
							<b>Abstract</b><p class="par_panel_perso">Abstract TBD </p>
						</div>
						<div class="modal-footer">
							<button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
						</div>
					</div>
				</div>
			</div>
		
	
		
	
		
			<div class="modal fade" id="talk_6" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-body">
							<b>Title:</b> (<b>KeyNote Talk</b>) Title TBD
							<br />
							<b>Presenter:</b> Prof. Christopher Re
							<br />
							
								<b>Bio</b><p class="par_panel_perso"><b>Christopher Re</b> is a Stanford AI Lab associate professor, he leads research on foundational AI with a focus on weak supervision and the interplay between AI and systems design. </p>
							
							<b>Abstract</b><p class="par_panel_perso">Abstract TBD </p>
						</div>
						<div class="modal-footer">
							<button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
						</div>
					</div>
				</div>
			</div>
		
	
		
			<div class="modal fade" id="talk_7" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-body">
							<b>Title:</b> (<b>KeyNote Talk</b>) Title TBD
							<br />
							<b>Presenter:</b> Dr. Navdeep Jaitly
							<br />
							
								<b>Bio</b><p class="par_panel_perso"><b>Navdeep Jaitly</b> worked under Geoffrey Hinton at the University of Toronto, his interest lie in pushing the frontier of Deep Learning research deep learning for Apple, following work on Google's Brain team. </p>
							
							<b>Abstract</b><p class="par_panel_perso">Abstract TBD </p>
						</div>
						<div class="modal-footer">
							<button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
						</div>
					</div>
				</div>
			</div>
		
	
		
			<div class="modal fade" id="talk_8" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-body">
							<b>Title:</b> (<b>KeyNote Talk</b>) Title TBD
							<br />
							<b>Presenter:</b> Prof. Danqi Chen
							<br />
							
								<b>Bio</b><p class="par_panel_perso"><b>Danqi Chen</b> co-leads Princeton's NLP Group, researches large language models, and emphasizes practicality and accessibility in AI development. </p>
							
							<b>Abstract</b><p class="par_panel_perso">Abstract TBD </p>
						</div>
						<div class="modal-footer">
							<button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
						</div>
					</div>
				</div>
			</div>
		
	
		
			<div class="modal fade" id="talk_9" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-body">
							<b>Title:</b> Accepted Oral Presentations (3)
							<br />
							<b>Presenter:</b> TBD
							<br />
							
								<b>Authors</b><p class="par_panel_perso">TBD </p>
							
							<b>Abstract</b><p class="par_panel_perso">Abstract TBD </p>
						</div>
						<div class="modal-footer">
							<button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
						</div>
					</div>
				</div>
			</div>
		
	
		
	
		
	
		
			<div class="modal fade" id="talk_12" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-body">
							<b>Title:</b> (<b>KeyNote Talk</b>) Title TBD
							<br />
							<b>Presenter:</b> Dr. Weizhu Chen
							<br />
							
								<b>Bio</b><p class="par_panel_perso"><b>Weizhu Chen</b> leads a modeling team in Microsoft Gen AI, working on large-scale (OpenAI and Microsoft) model training. </p>
							
							<b>Abstract</b><p class="par_panel_perso">Abstract TBD </p>
						</div>
						<div class="modal-footer">
							<button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
						</div>
					</div>
				</div>
			</div>
		
	
		
			<div class="modal fade" id="talk_13" tabindex="-1" aria-labelledby="exampleModalLabel" aria-hidden="true">
				<div class="modal-dialog">
					<div class="modal-content">
						<div class="modal-body">
							<b>Title:</b> (<b>KeyNote Talk</b>) Title TBD
							<br />
							<b>Presenter:</b> Prof. Hananeh Hajishirzi
							<br />
							
								<b>Bio</b><p class="par_panel_perso"><b>Hananeh Hajishirzi</b>, a leading NLP expert focusing on large language models, explores how AI can reason and understand complex information from various sources. </p>
							
							<b>Abstract</b><p class="par_panel_perso">Abstract TBD </p>
						</div>
						<div class="modal-footer">
							<button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
						</div>
					</div>
				</div>
			</div>
		
	
		
	
		
	
		
	
	<div class="row">
		<table id="customers">
			<tr>
				<th>Time</th>
				<th colspan="2">Title</th>
				<th>Presenter</th>
			</tr>
			<!-- insert table here -->
			
				<tr>
					<td>8:10AM - 8:15AM</td>
					
						<td colspan="3">Opening Speech</td>
					
				</tr>
			
				<tr>
					<td>8:15AM - 8:45AM</td>
					
						
							<td>(<b>KeyNote Talk</b>) Title TBD</td>
							<td>
								<center>
									<a href="" data-bs-toggle="modal" data-bs-target="#talk_2">
										<i class="bi bi-info-circle" style="font-size: 1.4rem;"></i>
									</a>
								</center>
							</td>
						
						<td><b>Prof. Maciej Besta</b></td>
					
				</tr>
			
				<tr>
					<td>8:45AM - 9:15AM</td>
					
						
							<td>(<b>KeyNote Talk</b>) Title TBD</td>
							<td>
								<center>
									<a href="" data-bs-toggle="modal" data-bs-target="#talk_3">
										<i class="bi bi-info-circle" style="font-size: 1.4rem;"></i>
									</a>
								</center>
							</td>
						
						<td><b>Dr. Peter Clark</b></td>
					
				</tr>
			
				<tr>
					<td>9:15AM - 10:00AM</td>
					
						
							<td>Accepted Oral Presentations (4)</td>
							<td>
								<center>
									<a href="" data-bs-toggle="modal" data-bs-target="#talk_4">
										<i class="bi bi-info-circle" style="font-size: 1.4rem;"></i>
									</a>
								</center>
							</td>
						
						<td><b>TBD</b></td>
					
				</tr>
			
				<tr>
					<td>10:00AM - 10:30AM</td>
					
						<td colspan="3">Morning Break</td>
					
				</tr>
			
				<tr>
					<td>10:30AM - 11:00AM</td>
					
						
							<td>(<b>KeyNote Talk</b>) Title TBD</td>
							<td>
								<center>
									<a href="" data-bs-toggle="modal" data-bs-target="#talk_6">
										<i class="bi bi-info-circle" style="font-size: 1.4rem;"></i>
									</a>
								</center>
							</td>
						
						<td><b>Prof. Christopher Re</b></td>
					
				</tr>
			
				<tr>
					<td>11:00AM - 11:30AM</td>
					
						
							<td>(<b>KeyNote Talk</b>) Title TBD</td>
							<td>
								<center>
									<a href="" data-bs-toggle="modal" data-bs-target="#talk_7">
										<i class="bi bi-info-circle" style="font-size: 1.4rem;"></i>
									</a>
								</center>
							</td>
						
						<td><b>Dr. Navdeep Jaitly</b></td>
					
				</tr>
			
				<tr>
					<td>11:30AM - 12:00AM</td>
					
						
							<td>(<b>KeyNote Talk</b>) Title TBD</td>
							<td>
								<center>
									<a href="" data-bs-toggle="modal" data-bs-target="#talk_8">
										<i class="bi bi-info-circle" style="font-size: 1.4rem;"></i>
									</a>
								</center>
							</td>
						
						<td><b>Prof. Danqi Chen</b></td>
					
				</tr>
			
				<tr>
					<td>12:00PM - 12:30PM</td>
					
						
							<td>Accepted Oral Presentations (3)</td>
							<td>
								<center>
									<a href="" data-bs-toggle="modal" data-bs-target="#talk_9">
										<i class="bi bi-info-circle" style="font-size: 1.4rem;"></i>
									</a>
								</center>
							</td>
						
						<td><b>TBD</b></td>
					
				</tr>
			
				<tr>
					<td>12:30PM - 1:15PM</td>
					
						<td colspan="3">Lunch Break</td>
					
				</tr>
			
				<tr>
					<td>1:15PM - 2:00PM</td>
					
						<td colspan="3"><b>Poster Session I </b> &amp; Free Discussion</td>
					
				</tr>
			
				<tr>
					<td>2:00PM - 2:30PM</td>
					
						
							<td>(<b>KeyNote Talk</b>) Title TBD</td>
							<td>
								<center>
									<a href="" data-bs-toggle="modal" data-bs-target="#talk_12">
										<i class="bi bi-info-circle" style="font-size: 1.4rem;"></i>
									</a>
								</center>
							</td>
						
						<td><b>Dr. Weizhu Chen</b></td>
					
				</tr>
			
				<tr>
					<td>2:30PM - 3:00PM</td>
					
						
							<td>(<b>KeyNote Talk</b>) Title TBD</td>
							<td>
								<center>
									<a href="" data-bs-toggle="modal" data-bs-target="#talk_13">
										<i class="bi bi-info-circle" style="font-size: 1.4rem;"></i>
									</a>
								</center>
							</td>
						
						<td><b>Prof. Hananeh Hajishirzi</b></td>
					
				</tr>
			
				<tr>
					<td>03:00PM - 03:15PM</td>
					
						<td colspan="3">Afternoon Break</td>
					
				</tr>
			
				<tr>
					<td>3:20PM - 4:10PM</td>
					
						
							<td colspan="2"><b>Interactive Panel Discussion</b></td>
						
						<td><b><ul><li>Dr. Marjan Ghazvini Nejad</li><li>Dr. Joel Hestness</li><li>Dr. Lu Hou</li></ul></b></td>
					
				</tr>
			
				<tr>
					<td>4:10PM-4:15PM</td>
					
						<td colspan="3">Best Paper and Poster Awards</td>
					
				</tr>
			
			<!-- -->
		</table>
	</div>

<!-- <div class="our24_timeline">
  <div class="our24_container our24_left">
    <div class="our24_date">15 Dec</div>
    <a href="" data-bs-toggle="modal" data-bs-target="#talk_3">
		<i class="icon bi bi-info-circle" style="font-size: 1.8rem;"></i>
	</a>
    <div class="our24_content">
      <h2 class="our24_h2">Lorem ipsum dolor sit amet</h2>
      <p class="our24_p">
        Lorem ipsum dolor sit amet elit. Aliquam odio dolor, id luctus erat sagittis non. Ut blandit semper pretium.
      </p>
    </div>
  </div>
  <div class="our24_container our24_right">
    <div class="our24_date">15 Dec</div>
    <a href="" data-bs-toggle="modal" data-bs-target="#talk_3">
		<i class="icon bi bi-info-circle" style="font-size: 1.8rem;"></i>
	</a>
    <div class="our24_content">
      <h2 class="our24_h2">Lorem ipsum dolor sit amet</h2>
      <p class="our24_p">
        Lorem ipsum dolor sit amet elit. Aliquam odio dolor, id luctus erat sagittis non. Ut blandit semper pretium. 
      </p>
    </div>
  </div>
  <div class="our24_container our24_left">
    <div class="our24_date">15 Dec</div>
    <a href="" data-bs-toggle="modal" data-bs-target="#talk_3">
		<i class="icon bi bi-info-circle" style="font-size: 1.8rem;"></i>
	</a>
    <div class="our24_content">
      <h2 class="our24_h2">Lorem ipsum dolor sit amet</h2>
      <p class="our24_p">
        Lorem ipsum dolor sit amet elit. Aliquam odio dolor, id luctus erat sagittis non. Ut blandit semper pretium.
      </p>
    </div>
  </div>
</div> -->
</p>

<!-- Organizers -->
<h2 class="blackpar_title" id="organizers">Organizers</h2>
<p>


	
	<div class="row_perso">
	
	
		
			
			<div class="card_perso column_perso" style="margin-left:13%;">
			  <img src="/images/Mehdi_Rezagholizadeh.jpg" alt="Mehdi Rezagholizadeh" class="img_card_perso" />
			  <div class="container_perso">
				<center>
				<h6>
					<b>Mehdi Rezagholizadeh</b>
					<br />
					Huawei Noah's Ark Lab
				</h6>
				<br />
				
					<a href="https://scholar.google.com/citations?user=MvXlF6kAAAAJ&amp;hl=en"><i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i></a>
				
				&nbsp;
				<a href="https://www.linkedin.com/in/mehdi-rezagholizadeh-61212346/"><i class="bi bi-linkedin" style="font-size: 2rem;"></i></a>
				</center>
			  </div>
			</div>
			
		
			
			<div class="card_perso column_perso">
			  <img src="/images/peyman_passban.jpg" alt="Peyman Passban" class="img_card_perso" />
			  <div class="container_perso">
				<center>
				<h6>
					<b>Peyman Passban</b>
					<br />
					BenchSci
				</h6>
				<br />
				
					<a href="https://scholar.google.ca/citations?user=wRaQX-EAAAAJ&amp;hl=en&amp;oi=ao"><i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i></a>
				
				&nbsp;
				<a href="https://www.linkedin.com/in/passban/?originalSubdomain=ca"><i class="bi bi-linkedin" style="font-size: 2rem;"></i></a>
				</center>
			  </div>
			</div>
			
		
			
			<div class="card_perso column_perso">
			  <img src="/images/Yue_Dong.jpg" alt="Yue Dong" class="img_card_perso" />
			  <div class="container_perso">
				<center>
				<h6>
					<b>Yue Dong</b>
					<br />
					University of California
				</h6>
				<br />
				
					<a href="https://scholar.google.ca/citations?user=WYkn4loAAAAJ"><i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i></a>
				
				&nbsp;
				<a href="https://www.linkedin.com/in/yuedongcs/"><i class="bi bi-linkedin" style="font-size: 2rem;"></i></a>
				</center>
			  </div>
			</div>
			
		
	
	</div>

	
	<div class="row_perso">
	
	
		
			
			<div class="card_perso column_perso" style="margin-left:13%;">
			  <img src="/images/yu_cheng.jfif" alt="Yu Cheng" class="img_card_perso" />
			  <div class="container_perso">
				<center>
				<h6>
					<b>Yu Cheng</b>
					<br />
					Microsoft
				</h6>
				<br />
				
					<a href="https://scholar.google.com/citations?user=ORPxbV4AAAAJ&amp;hl=en"><i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i></a>
				
				&nbsp;
				<a href="https://www.linkedin.com/in/chengyu05/"><i class="bi bi-linkedin" style="font-size: 2rem;"></i></a>
				</center>
			  </div>
			</div>
			
		
			
			<div class="card_perso column_perso">
			  <img src="/images/vahid_partovi.jpg" alt="Vahid Partovi Nia" class="img_card_perso" />
			  <div class="container_perso">
				<center>
				<h6>
					<b>Vahid Partovi Nia</b>
					<br />
					Huawei Noah's Ark Lab
				</h6>
				<br />
				
					<a href="https://scholar.google.ca/citations?user=onMDIN4AAAAJ&amp;hl=en&amp;oi=ao"><i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i></a>
				
				&nbsp;
				<a href="https://ca.linkedin.com/in/vahid-partovi-nia-29811385"><i class="bi bi-linkedin" style="font-size: 2rem;"></i></a>
				</center>
			  </div>
			</div>
			
		
			
			<div class="card_perso column_perso">
			  <img src="/images/boxing.jpg" alt="Boxing Chen" class="img_card_perso" />
			  <div class="container_perso">
				<center>
				<h6>
					<b>Boxing Chen</b>
					<br />
					Huawei Noah's Ark Lab
				</h6>
				<br />
				
					<a href="https://scholar.google.ca/citations?user=LiINs3gAAAAJ&amp;hl=en"><i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i></a>
				
				&nbsp;
				<a href="https://www.linkedin.com/in/boxingchen/"><i class="bi bi-linkedin" style="font-size: 2rem;"></i></a>
				</center>
			  </div>
			</div>
			
		
	
	</div>

</p>

<h2 class="blackpar_title" id="Organizers">Volunteers</h2>
<p>


	
	<div class="row_perso">
	
	
		
			
			<div class="card_perso column_perso" style="margin-left:13%;">
			  <img src="/images/khalil_bibi.png" alt="Khalil Bibi" class="img_card_perso" />
			  <div class="container_perso">
				<center>
				<h6>
					<b>Khalil Bibi</b>
					<br />
					
				</h6>
				<br />
				
					<a href="https://scholar.google.ca/citations?user=feQAvxoAAAAJ&amp;hl=en"><i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i></a>
				
				&nbsp;
				<a href="https://www.linkedin.com/in/khalilbibi/"><i class="bi bi-linkedin" style="font-size: 2rem;"></i></a>
				</center>
			  </div>
			</div>
			
		
			
			<div class="card_perso column_perso">
			  <img src="/images/dav.jpg" alt="David Alfonso-Hermelo" class="img_card_perso" />
			  <div class="container_perso">
				<center>
				<h6>
					<b>David Alfonso-Hermelo</b>
					<br />
					Huawei Noah's Ark Lab
				</h6>
				<br />
				
					<a href="https://scholar.google.ca/citations?user=g6GccGAAAAAJ&amp;hl=en&amp;oi=ao"><i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i></a>
				
				&nbsp;
				<a href="https://www.linkedin.com/in/david-alfonso-hermelo-6646a1b1/"><i class="bi bi-linkedin" style="font-size: 2rem;"></i></a>
				</center>
			  </div>
			</div>
			
		
			
			<div class="card_perso column_perso">
			  <img src="/images/soheila.jpg" alt="Soheila Samiee" class="img_card_perso" />
			  <div class="container_perso">
				<center>
				<h6>
					<b>Soheila Samiee</b>
					<br />
					BASF
				</h6>
				<br />
				
					<a href="https://scholar.google.ca/citations?user=9aZFWa8AAAAJ&amp;hl=en"><i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i></a>
				
				&nbsp;
				<a href="https://www.linkedin.com/in/soheila-samiee-3696a858/"><i class="bi bi-linkedin" style="font-size: 2rem;"></i></a>
				</center>
			  </div>
			</div>
			
		
	
	</div>

</p>
<!-- <div class="row_perso">
	<div class="card_perso column_perso justify-content-center" id="volunteer_card">
	  <img src="/images/khalil_bibi.png" alt="Khalil Bibi" class="img_card_perso">
	  <div class="container_perso" >
		<center>
		<h6>
			<b>Khalil Bibi</b>
			<br>
			Huawei Noah's Ark Lab
			<br>
			<a href="https://scholar.google.ca/citations?user=feQAvxoAAAAJ&hl=en">
				<i class="bi bi-mortarboard-fill" style="font-size: 2rem;"></i>
			</a>
			&nbsp;
			<a href="https://www.linkedin.com/in/khalilbibi/">
				<i class="bi bi-linkedin" style="font-size: 2rem;"></i>
			</a>
		</h6>
		</center>
	  </div>
	</div>
	<div class="card_perso column_perso justify-content-center" id="volunteer_card">
	  <img src="/images/dav.jpg" alt="Khalil Bibi" class="img_card_perso">
	  <div class="container_perso" >
		<center>
		<h6>
			<b>David Alfonso-Hermelo</b>
			<br>
			Huawei Noah's Ark Lab
			<br>
			<a href="https://scholar.google.ca/citations?user=g6GccGAAAAAJ&hl=en&oi=ao">
				<i class="da da-mortarboard-fill" style="font-size: 2rem;"></i>
			</a>
			&nbsp;
			<a href="https://www.linkedin.com/in/david-alfonso-hermelo-6646a1b1/">
				<i class="bi bi-linkedin" style="font-size: 2rem;"></i>
			</a>
		</h6>
		</center>
	  </div>
	</div>
</div> -->

<p><br /><br /></p>

<!-- Technical Committee -->
<h2 class="blackpar_title" id="technical_committee">Technical Committee</h2>
<p>


	
	<table>
		<tr>
			<td>
				<ul>
					
						
							<li> Dasgupta Sabyasachi (Sanofi) </li>
						
					
						
							<li> Tanya Roosta (Amazon) </li>
						
					
						
							<li> Peyman Passban (Sanofi) </li>
						
					
						
							<li> Hamidreza Saghir (Microsoft) </li>
						
					
						
							<li> Yue Dong (University of California, Riverside) </li>
						
					
						
							<li> Ruijiang Li (Sanofi) </li>
						
					
						
							<li> Abbas Ghaddar (Huawei Noah's Ark Lab) </li>
						
					
						
							<li> Alireza Ghaffari (McGill University) </li>
						
					
						
							<li> Yu Cheng (The Chinese University of Hong Kong) </li>
						
					
						
							<li> Jahangir Alam (Computer Research Institute of Montreal) </li>
						
					
						
							<li> Hamidreza Mahyar (McMaster University) </li>
						
					
						
							<li> Dan Alistarh (ISTA) </li>
						
					
						
							<li> Vahid Partovi Nia (Huawei Noah's Ark Lab) </li>
						
					
						
							<li> Soheila Samiee (BASF) </li>
						
					
						
							<li> Walid Ahmed (Technologies Canada) </li>
						
					
						
							<li> David Alfonso Hermelo (Huawei Noah's Ark Lab) </li>
						
					
						
							<li> Khalil Bibi (Huawei Noah's Ark Lab) </li>
						
					
						
							<li> Abbas Rahimi (IBM) </li>
						
					
						
							<li> Aysegul Bumin (Amazon) </li>
						
					
						
							<li> Abderrahim Fathan (Computer Research Institute of Montreal) </li>
						
					
						
							<li> Aref Jafari (University of Waterloo) </li>
						
					
						
							<li> Dan Fu (Stanford University, Canada) </li>
						
					
						
							<li> Parsa Omidi (Technologies Canada) </li>
						
					
						
							<li> Young Jin Kim (Microsoft) </li>
						
					
						
							<li> Giovanni Monea (EPFL) </li>
						
					
						
							<li> Mofetoluwa Adeyemi (University of Waterloo) </li>
						
					
						
							<li> Xindi Wang  (University of Western Ontario) </li>
						
					
						
							<li> Alessio Brutti (Fondazione Bruno Kessler) </li>
						
					
						
							<li> Saleh Ashkboos (ETH Zurich) </li>
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
				</ul>
			</td>
			<td>
				<ul>
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
					
						
							<li> Parsa Kavehzadeh (Huawei Noah's Ark Lab) </li>
						
					
						
							<li> Hossein Rajabzadeh (University of Waterloo) </li>
						
					
						
							<li> Mohammadreza Tayaranian (McGill University) </li>
						
					
						
							<li> Varun Gangal  (ASAPP Inc.) </li>
						
					
						
							<li> Sebastian Jaszczur (IDEAS NCBR, University of Warsaw) </li>
						
					
						
							<li> Ali Edalati (Huawei Noah's Ark Lab) </li>
						
					
						
							<li> Mojtaba Valipour (University of Waterloo) </li>
						
					
						
							<li> Heitor Guimarães (INRS University) </li>
						
					
						
							<li> Jing Li  (Mitsubishi Electric Research Laboratories) </li>
						
					
						
							<li> Mohammad Ruhul Amin (Fordham University) </li>
						
					
						
							<li> Mohammad Dehghan (Autodesk) </li>
						
					
						
							<li> Raffy Fahim (Microsoft) </li>
						
					
						
							<li> Feiyang Kang (Virginia Tech University) </li>
						
					
						
							<li> Ning Shi (University of Alberta) </li>
						
					
						
							<li> Daria Soboleva (Cerebras Systems) </li>
						
					
						
							<li> Qingru Zhang (Georgia Institute of Technology) </li>
						
					
						
							<li> Lilly Kumari (University of Washington) </li>
						
					
						
							<li> Thomas Ortner (IBM Research Zurich - Europe) </li>
						
					
						
							<li> Dominik Wagner  (Technische Hochschule Nuernberg) </li>
						
					
						
							<li> Benyamin Jamialahmadi (University of Waterloo) </li>
						
					
						
							<li> Tianshu Zhu (Huawei Noah's Ark Lab) </li>
						
					
						
							<li> Haoran Zhao (Drexel University &amp; University of Washington) </li>
						
					
						
							<li> Satya Sai Srinath Namburi (Amazon) </li>
						
					
						
							<li> Mouloud Belbahri (Layer 6 AI) </li>
						
					
						
							<li> Abhishek Panigrahi  (Princeton University) </li>
						
					
						
							<li> Arthur Pimentel (INRS) </li>
						
					
						
							<li> Mahsa Salmani (Huawei Technologies Canada) </li>
						
					
				</ul>
			</td>
		</tr>
	</table>

</p>
<p><br /><br /></p>

<!-- <h2 class="blackpar_title">Diamond Sponsors</h2> -->
<!-- <center>
	<img src="/images/logos.png">	
	<img src="/images/BASF_logo.png">	
</center> -->
<div class="row">
	<div class="col">
		<center>
			<img src="/images/huawei_logo.png" width="400px" />
		</center>
	</div>
	<div class="col">
		<center>
			<img src="/images/BASF_logo.png" width="250px" />
		</center>
	</div>	
	<div class="col">
		<center>
			<img src="/images/netmind_logo.png" width="500px" />
		</center>
	</div>
</div>
<p><br /><br />
<!-- <h2 class="blackpar_title">Platinum Sponsor</h2> -->
<!-- <div class="row">
	<div class="col">
		<center>
			<img src="/images/netmind_logo.png" width="450px">
		</center>
	</div>
</div>
<br><br> -->
<!-- <h2 class="blackpar_title">Gold Sponsor</h2> --></p>
<div class="row">
	<div class="col">
		<center>
			<img src="/images/shanghai_ai_lab1.png" width="250px" />
		</center>
	</div>
	<div class="col">
		<center>
			<img src="/images/Logo-Sanofi.png" width="220px" />
		</center>
	</div>
</div>



  </div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <ul class="contact-list">
          <li class="p-name">ENLSP NeurIPS Workshop 2024</li>
          <li><a class="u-email" href="mailto:neurips.ENLSP.2024@gmail.com">neurips.ENLSP.2024@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>ENLSP highlights some fundamental problems in NLP and speech  processing related to efficiency of the models, training and  inference for the general ML and DL communities.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>

<!-- Sponsors -->
<!-- <h2 class="par_title" id="sponsors">Our sponsors</h2> -->
<!--
<div class="row">
  <img class="column" src="/images/huawei.jpg">
    <img class="column" src="/images/darwin_ai.jpg">
  <img class="column" src="/images/huawei.jpg">
    <img class="column" src="/images/darwin_ai.jpg">
</div>--></div>

  </div>

</footer>
</body>

</html>
