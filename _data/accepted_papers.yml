[
{
"id": "1",
"title": "Snakes and Ladders: Accelerating State Space Model Inference with Speculative Decoding",
"abstract": "Speculative decoding is a method for accelerating inference in large language models (LLMs) by predicting multiple tokens using a smaller ‘draft model’ and validating them against the larger ‘base model.’ If a draft token is inconsistent with what the base model would have generated, speculative decoding ‘backtracks’ to the last consistent token before resuming generation. This is straightforward in autoregressive Transformer architectures since their state is a sliding window of past tokens. However, their baseline inference complexity is quadratic in the number of input tokens. State Space Models (SSMs) have linear inference complexity, but they maintain a separate Markov state that makes backtracking non-trivial. We propose two methods to perform speculative decoding in SSMs: “Joint Attainment and Advancement” and “Activation Replay.” Both methods utilize idle computational resources to speculate and verify multiple tokens, allowing us to produce 6 tokens for 1.47⇥ the cost of one, corresponding to an average 1.82⇥ wall-clock speed-up on three different benchmarks using a simple n-gram for drafting. Furthermore, as model size increases, relative overhead of speculation and verification decreases: Scaling from 1.3B parameters to 13B reduces relative overhead from 1.98⇥ to 1.22⇥. Unlike Transformers, speculative decoding in SSMs can be easily applied to batches of sequences, allowing dynamic allocation of resources to fill gaps in compute utilization and thereby improving efficiency and throughput with variable inference traffic.",
"authors": "Yangchao Wu1, Yonatan Dukler2, Matthew Trager, Wei Xia, Alessandro Achille, Stefano Soatto",
"has_supp": true,
},
{
"id": "2",
"title": "AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language Models via an Entropy-based Lower Bound on Token Acceptance Probability",
"abstract": "Speculative decoding is a powerful technique that attempts to circumvent the autoregressive constraint of modern Large Language Models (LLMs). The aim of speculative decoding techniques is to improve the average inference time of a large, target model without sacrificing its accuracy, by using a more efficient draft model to propose draft tokens which are then verified in parallel. The number of draft tokens produced in each drafting round is referred to as the draft length and is often a static hyperparameter chosen based on the acceptance rate statistics of the draft tokens. However, setting a static draft length can negatively impact performance, especially in scenarios where drafting is expensive and there is a high variance in the number of tokens accepted. Adaptive Entropy-based Draft Length (AdaEDL) is a simple, training and parameter-free criteria which allows for early stopping of the token drafting process by approximating a lower bound on the expected acceptance probability of the drafted token based on the currently observed entropy of the drafted logits. We show that AdaEDL consistently outperforms static draft-length speculative decoding by 10%-57% as well as other training-free draft-stopping techniques by upto 10% in a variety of settings and datasets. At the same time, we show that AdaEDL is more robust than these techniques and preserves performance in high-sampling-temperature scenarios. Since it is training-free, in contrast to techniques that rely on the training of dataset-specific draft-stopping predictors, AdaEDL can seamlessly be integrated into a variety of pre-existing LLM systems.",
"authors": "Sudhanshu Agrawal, Wonseok Jeon, Mingu Lee",
"has_supp": false,
},
{
"id": "3",
"title": "GEAR: An Efficient Error Reduction Framework for KV Cache Compression in LLM Inference",
"abstract": "Key-value (KV) caching has become the de-facto technique to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing entries group-wise. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient error reduction framework that augments a quantization scheme with two error reduction components and achieves near-lossless performance at high compression ratios. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low-rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments show that GEAR can maintain similar accuracy to that of FP16 cache with improvement up to 24.42% over the SOTA baselines at 2-bit compression. Additionally, compared to LLM inference with FP16 KV cache, GEAR can reduce peak-memory of up to 2.39×, bringing 2.1× ∼ 5.07× throughput improvement. Our code will be publicly available.",
"authors": "Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao",
"has_supp": false,
},
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
# {
# "id": "",
# "title": "",
# "abstract": "",
# "authors": "",
# "has_supp": false,
# },
]
