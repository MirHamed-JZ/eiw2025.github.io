[
{
"id": "1",
"title": "Snakes and Ladders: Accelerating State Space Model Inference with Speculative Decoding",
"abstract": "Speculative decoding is a method for accelerating inference in large language models (LLMs) by predicting multiple tokens using a smaller ‘draft model’ and validating them against the larger ‘base model.’ If a draft token is inconsistent with what the base model would have generated, speculative decoding ‘backtracks’ to the last consistent token before resuming generation. This is straightforward in autoregressive Transformer architectures since their state is a sliding window of past tokens. However, their baseline inference complexity is quadratic in the number of input tokens. State Space Models (SSMs) have linear inference complexity, but they maintain a separate Markov state that makes backtracking non-trivial. We propose two methods to perform speculative decoding in SSMs: “Joint Attainment and Advancement” and “Activation Replay.” Both methods utilize idle computational resources to speculate and verify multiple tokens, allowing us to produce 6 tokens for 1.47⇥ the cost of one, corresponding to an average 1.82⇥ wall-clock speed-up on three different benchmarks using a simple n-gram for drafting. Furthermore, as model size increases, relative overhead of speculation and verification decreases: Scaling from 1.3B parameters to 13B reduces relative overhead from 1.98⇥ to 1.22⇥. Unlike Transformers, speculative decoding in SSMs can be easily applied to batches of sequences, allowing dynamic allocation of resources to fill gaps in compute utilization and thereby improving efficiency and throughput with variable inference traffic.",
"authors": "Yangchao Wu1, Yonatan Dukler2, Matthew Trager, Wei Xia, Alessandro Achille, Stefano Soatto",
"has_supp": true,
},
{
"id": "3",
"title": "GEAR: An Efficient Error Reduction Framework for KV Cache Compression in LLM Inference",
"abstract": "Key-value (KV) caching has become the de-facto technique to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing entries group-wise. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient error reduction framework that augments a quantization scheme with two error reduction components and achieves near-lossless performance at high compression ratios. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low-rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments show that GEAR can maintain similar accuracy to that of FP16 cache with improvement up to 24.42% over the SOTA baselines at 2-bit compression. Additionally, compared to LLM inference with FP16 KV cache, GEAR can reduce peak-memory of up to 2.39×, bringing 2.1× ∼ 5.07× throughput improvement. Our code will be publicly available.",
"authors": "Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao",
"has_supp": false,
},
{
"id": "4",
"title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
"abstract": "Foundation models (FMs) are pre-trained on large-scale datasets and then fine- tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to modulate the pre-trained weights via a low-rank adaptation (LoRA) of newly introduced weights. These weight matrices are usually initialized at random with the same rank for each layer across the FM, which results in suboptimal performance. We propose to enhance LoRA by initializing the new weights in a data-driven manner, by computing singular value decomposition on activation vectors. Then, we initialize the new LoRA matrices with the obtained right-singular vectors. Finally, we re-distribute the ranks among layers to explain the maximal amount of variance across all layers. This assignment results in an adaptive allocation of ranks per weight matrix, and inherits all benefits of LoRA. We apply our new method, Explained Variance Adaptation (EVA), to a variety of fine-tuning tasks comprising language understanding and generation, image classification, and reinforcement learning. EVA consistently attains the highest average score across a multitude of tasks per domain.",
"authors": "Fabian Paischer, Lukas Hauzenberger, Thomas Schmied",
"has_supp": false,
},
{
"id": "6",
"title": "Rephrasing natural text data with different languages and quality levels for Large Language Model pre-training",
"abstract": "Recently published work on rephrasing natural text data for pre-training LLMs has shown promising results when combining the original dataset with the synthetically rephrased data. We build upon previous work by replicating existing results on C4 and extending them with our optimized rephrasing pipeline to the English, German, Italian, and Spanish Oscar subsets of CulturaX. Our pipeline leads to increased performance on stan- dard evaluation benchmarks in both the mono- and multilingual setup. In addition, we provide a detailed study of our pipeline, investigating the choice of the base dataset and LLM for the rephrasing, as well as the relationship between the model size and the performance after pre-training. By exploring data with different perceived quality levels, we show that gains decrease with higher quality. Furthermore, we find the difference in performance between model families to be bigger than between different model sizes. This highlights the necessity for detailed tests before choosing an LLM to rephrase large amounts of data. Moreover, we investigate the effect of pre-training with synthetic data on supervised fine-tuning. Here, we find in- creasing but inconclusive results that highly depend on the used benchmark. These results (again) highlight the need for better benchmarking setups. In summary, we show that rephrasing multilingual and low-quality data is a very promising direction to extend LLM pre-training data.",
"authors": "Michael Pieler, Marco Bellagente, Hannah Teufel, Duy Phung",
"has_supp": false,
},
{
"id": "7",
"title": "Post-Training Statistical Calibration for Higher Activation Sparsity",
"abstract": "We present Statistical Calibrated Activation Pruning (SCAP), a post-training acti- vation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5× additional LLM decoding speedup against CATS[12] at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, high- lighting its practicality and scalability. The code is available here.",
"authors": "Vui Seng Chua, Yujie Pan, Nilesh Jain",
"has_supp": false,
},
{
"id": "8",
"title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
"abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory con- sumption during inference. Unlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights. In response, we propose T HIN K, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20% compared with vanilla KV cache eviction and quantization methods. For instance, T HIN K integrated with KIVI can achieve a 2.8× reduction in peak memory usage while maintaining nearly the same quality, enabling up to a 5× increase in batch size when using a single GPU. Extensive eval- uations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of T HIN K, establishing a new baseline algorithm for efficient LLM deployment without compromising performance.",
"authors": "Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, Doyen Sahoo",
"has_supp": false,
},
{
"id": "9",
"title": "BiRNA-BERT: Adaptive Tokenization for Efficient RNA Language Modeling",
"abstract": "Recent advancements in Transformer-based language models have spurred interest, in their use for biological sequence analysis. However, adapting models like BERT, is challenging due to sequence length, often requiring truncation for proteomics, and genomics tasks. Additionally, advanced tokenization and relative positional, encoding techniques for long contexts in NLP are often not directly transferable, to DNA/RNA sequences, which require nucleotide or character-level encodings, for tasks such as 3D torsion angle prediction, distance map prediction or sec-, ondary structure prediction. To tackle these challenges, we propose an adaptive, dual tokenimzation scheme for bioinformatics that utilizes both nucleotide-level, (NUC) and efficient BPE tokenizations. Building on the dual tokenization, we, introduce BiRNA-BERT, a 117M parameter Transformer encoder pretrained with, our proposed tokenization on 28 billion nucleotides across 36 million coding, and non-coding RNA sequences. The learned representation by BiRNA-BERT, generalizes across a range of applications. The BiRNA-BERT model achieves, state-of-the-art results in long-sequence downstream tasks, performs comparably, well in short-sequence tasks, and matches the performance in nucleotide-level, structural prediction tasks, of models six times larger in parameter size, while, requiring 27 times less pre-training compute. In addition, our empirical experi-, ments and ablation studies demonstrate that NUC is often preferable over BPE, for bioinformatics tasks, given sufficient VRAM availability. We further demon-, strate the applicability of the dual-pretraning and adaptive tokenization strategy, employing this concept on a DNA language model which provides comparable, performance to 66X compute heavy DNA language models. BiRNA-BERT can, dynamically adjust its tokenization strategy based on sequence lengths, utilizing, NUC for shorter sequences and switching to BPE for longer ones, thereby offering, for the first time, the capability to efficiently handle arbitrarily long DNA/RNA sequences.",
"authors": "Md Toki Tahmid, Haz Sameen Shahgir, Sazan Mahbub, Yue Dong, Md. Shamsuzzoha Bayzid",
"has_supp": false,
},
{
"id": "11",
"title": "Disentangling Questions from Query Generation for Task-Adaptive Retrieval",
"abstract": "This paper studies the problem of information retrieval, to adapt to unseen tasks., Existing work generates synthetic queries from domain-specific documents to, jointly train the retriever. However, the conventional query generator assumes the, query as a question, thus failing to accommodate general search intents. A more, lenient approach incorporates task-adaptive elements, such as few-shot learning, with an 137B LLM. In this paper, we challenge a trend equating query and question,, and instead conceptualize query generation task as a “compilation” of high-level, intent into task-adaptive query. Specifically, we propose EGG, a query generator, that better adapts to wide search intents expressed in the BeIR benchmark. Our, method outperforms baselines and existing models on four tasks with underexplored, intents, while utilizing a query generator 47 times smaller than the previous state-, of-the-art. Our findings reveal that instructing the LM with explicit search intent is, a key aspect of modeling an effective query generator.",
"authors": "Yoonsang Lee, Minsoo Kim, Seung-won Hwang",
"has_supp": false,
},
{
"id": "13",
"title": "The N-Grammys: Accelerating Autoregressive Inference with Learning-Free Batched Speculation",
"abstract": "Speculative decoding aims to speed up autoregressive generation of a language, model by verifying in parallel the tokens generated by a smaller draft model. In, this work, we explore the effectiveness of learning-free, negligible-cost draft strate-, gies, namely N -grams obtained from the model weights and the context. While, the predicted next token of the base model is rarely the top prediction of these, simple strategies, we observe that it is often within their top-k predictions for small, k. Based on this, we show that combinations of simple strategies can achieve, significant inference speedups over different tasks. The overall performance is, comparable to more complex methods, yet does not require expensive preprocess-, ing or modification of the base model, and allows for seamless ‘plug-and-play’, integration into pipelines.",
"authors": "Lawrence Stewart, Matthew Trager, Sujan Kumar Gonugondla, Stefano Soatto",
"has_supp": false,
},
{
"id": "14",
"title": "Different Rates for Different Weights: Decoupled Relative Learning Rate Schedules",
"abstract": "In this work, we introduce a novel approach for optimizing neural network training, by adjusting learning rates across weights of different components in Transformer, models. Traditional methods often apply a uniform learning rate across all network, layers, potentially overlooking the unique dynamics of each part. Remarkably,, our introduced Relative Learning Rate Schedules (RLRS) method accelerates the, training process by up to 23%, particularly in complex models such as Mixture, of Experts (MoE). Hyperparameters of RLRS can be efficiently tuned on smaller, models and then extrapolated to 27× larger ones. This simple and effective method, results in a substantial reduction in training time and computational resources,, offering a practical and scalable solution for optimizing large-scale neural networks.",
"authors": "Jan Ludziejewski, Jan Małaśnicki, Maciej Pióro, Michał Krutul, Kamil Ciebiera, Maciej Stefaniak, Jakub Krajewski, Piotr Sankowski, Marek Cygan, Kamil Adamczewski, Sebastian Jaszczur",
"has_supp": false,
},
{
"id": "15",
"title": "Distributed Speculative Inference of Large Language Models",
"abstract": "Accelerating the inference of large language models (LLMs) is an important, challenge in artificial intelligence. This paper introduces distributed speculative, inference (DSI), a novel distributed inference algorithm that is provably faster than, speculative inference (SI) [Leviathan et al., 2023, Chen et al., 2023, Miao et al.,, 2023] and traditional autoregressive inference (non-SI). Like other SI algorithms,, DSI works on frozen LLMs, requiring no training or architectural modifications,, and it preserves the target distribution. Prior studies on SI have demonstrated, empirical speedups (compared to non-SI) but require a fast and accurate drafter, LLM. In practice, off-the-shelf LLMs often do not have matching drafters that are, sufficiently fast and accurate. We show a gap: SI gets slower than non-SI when, using slower or less accurate drafters. We close this gap by proving that DSI is, faster than both SI and non-SI—given any drafters. By orchestrating multiple, instances of the target and drafters, DSI is not only faster than SI but also supports, LLMs that cannot be accelerated with SI. Our simulations show speedups of off-, the-shelf LLMs in realistic settings: DSI is 1.29-1.92x faster than SI. Our code is, open-sourced: github.com/keyboardAnt/Distributed-Speculative-Inference",
"authors": "Nadav Timor, Jonathan Mamoui, Daniel Korati, Moshe Berchanskyi, Oren Peregi, Moshe Wasserblati, Tomer Galanti, Michal Gordon, David Harel",
"has_supp": false,
},
{
"id": "16",
"title": "XC-C ACHE: Cross-Attending to Cached Context for Efficient LLM Inference",
"abstract": "Prompts are often employed to condition decoder-only language model generation, on reference information. Just-in-time processing of a context is inefficient due to, the quadratic cost of self-attention operations, and caching is desirable. However,, caching transformer states can easily require almost as much space as the model, parameters. When the right context is not known in advance, caching the prompt, can be challenging. This work addresses these limitations by introducing models, that, inspired by the encoder-decoder architecture, use cross-attention to condition, generation on reference text without the prompt. More precisely, we leverage, pre-trained decoder-only models and only train a small number of added layers., We use Question-Answering (QA) as a testbed to evaluate the ability of our models, to perform conditional generation and observe that they outperform prompt-based, inference methods, are comparable to fine-tuned prompted LLMs, and drasti-, cally reduce the space footprint relative to standard KV caching by two orders of, magnitude. Specifically, we introduced XC-L LAMA which converts a pre-trained, L LAMA 2 into an encoder-decoder architecture by integrating cross-attention layers, interleaved in between existing self-attention layers.",
"authors": "João Monteiro, Étienne Marcotte, Pierre-André Noël, Valentina Zantedeschi, David Vázquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian",
"has_supp": false,
},
{
"id": "17",
"title": "Text Summarization With Graph Attention Networks",
"abstract": "This study aimed to leverage graph information, particularly Rhetorical Structure, Theory (RST) and Co-reference (Coref) graphs, to enhance the performance of, our baseline summarization models. Specifically, we experimented with a Graph, Attention Network architecture to incorporate graph information. However, this, architecture did not enhance the performance. Subsequently, we used a simple, Multi-layer Perceptron architecture, which improved the results in our proposed, model on our primary dataset, CNN/DM. Additionally, we annotated XSum dataset, with RST graph information, establishing a benchmark for future graph-based, summarization models. This secondary dataset posed multiple challenges, revealing, both the merits and limitations of our models.",
"authors": "Mohammadreza Ardestani, Yllias Chali",
"has_supp": false,
},
{
"id": "18",
"title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
"abstract": "Self-supervised speech representation models, particularly those leveraging trans-, former architectures, have demonstrated remarkable performance on downstream, tasks. Recent studies revealed high redundancy of transformer layers, potentially, allowing for smaller models and more efficient inference. We perform a detailed, analysis of layer similarity in speech models, leveraging three similarity metrics., Our findings reveal a block-like structure of high similarity, suggesting significant, redundancy within the blocks along with two main processing steps that are both, found to be critical for maintaining performance. We demonstrate the effectiveness, of pruning transformer-based speech models without post-training, achieving up, to 40% reduction in transformer layers while maintaining 95% of the model’s, predictive capacity. Lastly, we show that replacing the transformer stack with a few, simple layers can reduce the network size by up to 95% and inference time by up to, 87%, significantly reducing the computational footprint with minimal performance, loss, revealing the benefits of model simplification for downstream applications.",
"authors": "Albert Kjøller Jacobsen, Teresa Dorszewski, Lenka Tětková, Lars Kai Hansen",
"has_supp": false,
},
{
"id": "19",
"title": "Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models",
"abstract": "Speculative decoding is commonly used for reducing the inference latency of, large language models. Its effectiveness depends highly on the speculation looka-, head (SL)—the number of tokens generated by the draft model at each iteration. In, this work we show that the common practice of using the same SL for all iterations, (static SL) is suboptimal. We introduce DISCO (DynamIc SpeCulation lookahead, Optimization), a novel method for dynamically selecting the SL. Our experiments, with four datasets show that DISCO reaches an average speedup of 10% compared, to the best static SL baseline, while generating the exact same text.",
"authors": "Jonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy Schwartz",
"has_supp": false,
},
{
"id": "20",
"title": "AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language Models via an Entropy-based Lower Bound on Token Acceptance Probability",
"abstract": "Speculative decoding [1] is a powerful technique that attempts to circumvent the, autoregressive constraint of modern Large Language Models (LLMs). The aim of, speculative decoding techniques is to improve the average inference time of a large,, target model without sacrificing its accuracy, by using a more efficient draft model, to propose draft tokens which are then verified in parallel. The number of draft, tokens produced in each drafting round is referred to as the draft length and is often, a static hyperparameter chosen based on the acceptance rate statistics of the draft, tokens. However, setting a static draft length can negatively impact performance,, especially in scenarios where drafting is expensive and there is a high variance in the, number of tokens accepted. Adaptive Entropy-based Draft Length (AdaEDL) is a, simple, training and parameter-free criteria which allows for early stopping of the, token drafting process by approximating a lower bound on the expected acceptance, probability of the drafted token based on the currently observed entropy of the, drafted logits. We show that AdaEDL consistently outperforms static draft-length, speculative decoding by 10%-57% as well as other training-free draft-stopping, techniques by upto 10% in a variety of settings and datasets. At the same time, we, show that AdaEDL is more robust than these techniques and preserves performance, in high-sampling-temperature scenarios. Since it is training-free, in contrast to, techniques that rely on the training of dataset-specific draft-stopping predictors,, AdaEDL can seamlessly be integrated into a variety of pre-existing LLM systems.",
"authors": "Sudhanshu Agrawal, Wonseok Jeon, Mingu Lee",
"has_supp": false,
},
{
"id": "21",
"title": "OnlySportsLM: Optimizing Sports-Domain Language Models with SOTA Performance under Billion Parameters",
"abstract": "This paper explores the potential of a small, domain-specific language model trained, exclusively on sports-related data. We investigate whether extensive training data, with specially designed small model structures can overcome model size constraints., The study introduces the OnlySports collection, comprising OnlySportsLM,, OnlySports Dataset, and OnlySports Benchmark. Our approach involves:, 1) creating a massive 600 billion tokens OnlySports Dataset from FineWeb,, 2) optimizing the RWKV-v6 architecture for sports-related tasks, resulting in a, 196M parameters model with 20-layer, 640-dimension structure, 3) training the, OnlySportsLM on part of OnlySports Dataset, and 4) testing the resultant, model on OnlySports Benchmark. OnlySportsLM achieves a 37.62%/34.08%, accuracy improvements over previous 135M/360M state-of-the-art models and, matches the performance of larger models such as SomlLM 1.7B and Qwen, 1.5B in the sports domain. Additionally, the OnlySports collection presents, a comprehensive workflow for building high-quality, domain-specific language, models, providing a replicable blueprint for efficient AI development across various, specialized fields.",
"authors": "Zexin Chen, Chengxi Li, Xiangyu Xie, Parijat Dube",
"has_supp": false,
},
{
"id": "22",
"title": "Dense Backpropagation Improves Routing for Sparsely-Gated Mixture-of-Experts",
"abstract": "Mixture of Experts (MoE) pretraining is more scalable than dense Trans-, former pretraining, because MoEs learn to route inputs to a sparse set of, their feedforward parameters. However, this means that MoEs only receive, a sparse backward update, leading to problems such as router load imbal-, ance where some experts receive more tokens than others. We present a, lightweight approximation method that gives the MoE a dense gradient, while only sparsely activating its parameters. A key insight into the design, of our method is that at scale, many tokens not routed to a given expert, may nonetheless lie in the span of tokens that were routed to that expert,, allowing us to create an approximation for the expert output of that token, from existing expert outputs. Our dense backpropagation outperforms stan-, dard TopK routing across multiple MoE configurations without increasing, runtime.",
"authors": "Ashwinee Panda, Vatsal Baherwani, Benjamin Thérien, Stephen Rawls, Sambit Sahu, Supriyo Chakraborty, Tom Goldstein",
"has_supp": false,
},
{
"id": "23",
"title": "Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection",
"abstract": "Large language models (LLMs) augmented with retrieval exhibit robust perfor-, mance and extensive versatility by incorporating external contexts. However,, the input length grows linearly in the number of retrieved documents, causing a, dramatic increase in latency. In this paper, we propose a novel paradigm named, Sparse RAG, which seeks to cut computation costs through sparsity. Specifically,, Sparse RAG encodes retrieved documents in parallel, which eliminates latency, introduced by long-range attention of retrieved documents. Then, LLMs selectively, decode the output by only attending to highly relevant caches auto-regressively,, which are chosen via prompting LLMs with special control tokens. It is notable, that Sparse RAG combines the assessment of each individual document and the, generation of the response into a single process. The designed sparse mechanism, in a RAG system can facilitate the reduction of the number of documents loaded, during decoding for accelerating the inference of the RAG system. Additionally,, filtering out undesirable contexts enhances the model’s focus on relevant context,, inherently improving its generation quality. Evaluation results of two datasets show, that Sparse RAG can strike an optimal balance between generation quality and, computational efficiency, demonstrating its generalizability across both short- and, long-form generation tasks.",
"authors": "Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, Jindong Chen",
"has_supp": false,
},
{
"id": "24",
"title": "VL-Mamba: Exploring State Space Models for Multimodal Learning",
"abstract": "Multimodal large language models (MLLMs) have gained considerable atten-, tion due to their ability to integrate visual and textual information, enhancing, understanding and providing context for complex tasks. While Transformer-based, architectures have been the dominant framework for MLLMs, recent studies sug-, gest that state space models (SSMs) like Mamba can achieve competitive or even, superior performance. However, no prior research has investigated the potential, of SSMs to replace Transformers in multimodal tasks, which are inherently more, challenging due to the heterogeneity of visual and language data and the com-, plexities of aligning these modalities. In this paper, we introduce VL-Mamba, the, first study to explore the application of state space models in multimodal learning, tasks. VL-Mamba leverages a pretrained Mamba language model as its core, and, we propose a novel MultiModal Connector (MMC) that incorporates a Vision, Selective Scan (VSS) module to improve visual sequence modeling. We empir-, ically explore how to effectively apply the 2D vision selective scan mechanism, for multimodal learning and the combinations of different vision encoders and, variants of pretrained Mamba language models. Our experiments across multiple, multimodal benchmarks demonstrate that VL-Mamba achieves competitive perfor-, mance against small MLLMs of similar size, and in some cases, surpasses larger, models such as the 7B and 13B versions of LLaVA-1.5. These results suggest that, state space models have the potential to serve as an alternative to Transformers in, multimodal learning tasks.",
"authors": "Yanyuan Qiao, Zheng Yu, Zijia Zhao, Sihan Chen, Mingzhen Sun, Longteng Guo, Qi Wu, Jing Liu",
"has_supp": false,
},
{
"id": "27",
"title": "Composite Attention: A Framework for Combining Sequence Mixing Primitives",
"abstract": "Hybrid attention architectures have shown promising success in both equipping, self attention with inductive bias for long-sequence modelling and reducing the, computational burden of transformers without sacrificing quality. This paper intro-, duces Composite Attention, a theoretical framework for analyzing the combination, of sequence mixing primitives in modern deep learning architectures. Utilizing the, definition of sequence mixers as structured linear maps, we formalize the composi-, tion of sequence mixing primitives as either sequential or recurrent composition.",
"authors": "Harry Jake Cunningham, Marc Peter Deisenroth",
"has_supp": false,
},
{
"id": "28",
"title": "Inference-Friendly Models With MixAttention",
"abstract": "The size of the key-value (KV) cache plays a critical role in determining both the, maximum context length and the number of concurrent requests supported during, inference in modern language models. The KV cache size grows proportionally, with the number of attention heads and the tokens processed, leading to increased, memory consumption and slower inference for long inputs. In this work, we, explore the use of MixAttention, a model architecture modification closely related, to a blog published by Character.AI [Character.AI, 2024]. MixAttention combines, sliding window attention, where only a small subset of recent tokens is stored in, the KV cache, with KV cache sharing across layers. Our experiments demonstrate, that MixAttention significantly reduces memory usage and improves inference, speed without sacrificing model performance in both short and long-context tasks., We also explore various configurations of this architecture, identifying those that, maintain quality across evaluation metrics while optimizing resource efficiency.",
"authors": "Shashank Rajput, Sean Owen, Ying Sheng, Vitaliy Chiley",
"has_supp": false,
},
{
"id": "32",
"title": "On the Efficiency of NLP-Inspired Methods for Tabular Deep Learning",
"abstract": "Recent advancements in tabular deep learning (DL) have led to substantial per-, formance improvements, surpassing the capabilities of traditional models. With, the adoption of techniques from natural language processing (NLP), such as lan-, guage model-based approaches, DL models for tabular data have also grown in, complexity and size. Although tabular datasets do not typically pose scalability, issues, the escalating size of these models has raised efficiency concerns. De-, spite its importance, efficiency has been relatively underexplored in tabular DL, research. This paper critically examines the latest innovations in tabular DL, with, a dual focus on performance and computational efficiency. The source code is, available at https://github.com/basf/mamba-tabular.",
"authors": "Anton Frederik Thielmann, Soheila Samiee",
"has_supp": false,
},
{
"id": "33",
"title": "OLMOE: Open Mixture-of-Experts Language Models",
"abstract": "We introduce OLMOE, a fully open, state-of-the-art language model leveraging, sparse Mixture-of-Experts (MoE). OLMOE-1B-7B has 7 billion (B) parameters, but uses only 1B per input token. We pretrain it on 5 trillion tokens and further, adapt it to create OLMOE-1B-7B-INSTRUCT. Our models outperform all available, models with similar active parameters, even surpassing larger ones like Llama2-, 13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training,, define and analyze new routing properties showing high specialization in our model,, and open-source all our work: model weights, training data, code, and logs.",
"authors": "Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi",
"has_supp": false,
},
{
"id": "34",
"title": "Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization",
"abstract": "The pre-training phase of language models often begins with randomly initialized, parameters. With the current trends in scaling models, training their large number, of parameters can be extremely slow and costly. In contrast, small language models, are less expensive to train, but they often cannot achieve the accuracy of large, models. In this paper, we explore an intriguing idea to connect these two different, regimes: Can we develop a method to initialize large language models using, smaller pre-trained models? Will such initialization bring any benefits in terms, of training time and final accuracy? In this paper, we introduce HyperCloning, a, method that can expand the parameters of a pre-trained language model to those, of a larger model with increased hidden dimensions. Our method ensures that, the larger model retains the functionality of the smaller model. As a result, the, larger model already inherits the predictive power and accuracy of the smaller, model before the training starts. We demonstrate that training such an initialized, model results in significant savings in terms of GPU hours required for pre-training, large language models. Implementation of HyperCloning is available at https:, //github.com/apple/ml-hypercloning/tree/main.",
"authors": "Mohammad Samragh, Minsik Cho, Iman Mirzadeh, Moin Nabi, Keivan Alizadeh Vahid, Devang Naik, Fartash Faghri, Mehrdad Farajtabar",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
{
"id": "",
"title": "",
"abstract": "",
"authors": "",
"has_supp": false,
},
]
