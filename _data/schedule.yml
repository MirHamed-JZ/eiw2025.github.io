- Title: Breakfast 
  Time: 8:00AM - 8:15AM
  Whole_line: true

- Title: Opening Speech
  Time: 8:15AM - 8:30AM
  Whole_line: true  

- Title: (<b>KeyNote Talk</b>) Efficiency through Learning from Experience
  Time: 8:30AM - 9:00AM
  Presenter: Dr. Bhavana Dalvi Mishra
  Is_paper: false
  Whole_line: false
  Bio: <b>Dr. Bhavana Dalvi Mishra</b> is a Lead Research Scientist at the Allen Institute for AI (Ai2). Her research focuses on NLP, interactive reasoning, and scientific discovery. She obtained her Ph.D. in Computer Science from Carnegie Mellon University in 2015 and earned her Master's in Computer Science from the Indian Institute of Technology, Bombay in 2007. She has received several awards, including two Best Paper runner-up awards, Google Ph.D. fellowship, and Barbara Lazarus Women@IT Fellowship from CMU for her contributions to NLP and AI.
  Abstract: Despite the physiological limitations of the human brain, humans are remarkably efficient thinkers, in large part because they can learn from experience, allowing them to avoid prior reasoning errors and quickly jump to conclusions that previously took substantial effort. Similarly, language models (LMs) can rapidly improve their inference-time efficiency through inference-time learning, supplementing lower-level methods like fast decoding and caching. I'll describe two agent-based systems (CLIN and SSO) that do this, using an external RAG (retrieval-augmented generation) memory to help the agent navigate a complex, virtual environment. Unlike typical RAG systems, the memory is dynamic and updated after each task (including forgetting unhelpful learnings). In addition, unlike reinforcement-based continual learning techniques, these systems rapidly learn from just a handful of examples by exploiting LMs to conjecture useful generalizations of past experiences.  I'll outline three critical activities in this process - what to remember, how to index those memories, and how to retrieve from that index - and how those choices impact the effectiveness of the resulting agent. While this concept of efficiency is a little different to foundational architectural considerations, I'll show that it is nonetheless powerful, and an important additional tool in the toolbox for efficient future applications.

- Title: (<b>KeyNote Talk</b>) Multi-Teacher Distillation&#58; An Ensemble-Then-Distill Approach
  Time: 9:00AM - 9:30AM
  Presenter: Prof. Lili Mou
  Is_paper: false
  Whole_line: false
  Bio: <b> Dr. Lili Mou</b> is an Assistant Professor at the Department of Computing Science, University of Alberta. He is also an Alberta Machine Intelligence Institute (Amii) Fellow and a Canada CIFAR AI (CCAI) Chair. Lili received his BS and PhD degrees in 2012 and 2017, respectively, from School of EECS, Peking University. After that, he worked as a postdoctoral fellow at the University of Waterloo. His research interests mainly lie in designing novel machine learning algorithms and frameworks for NLP. He has publications at top conferences and journals, including ACL, EMNLP, TACL, ICML, ICLR, and NeurIPS. He also presented tutorials at EMNLP'19 and ACL'20. He received a AAAI New Faculty Highlight Award in 2021.
  Abstract: Knowledge distillation (KD) aims to transfer the knowledge in a large model (called a teacher) into a small one (called a student), and has become an emerging research topic as the sizes of deep learning models keep growing. Today, there are abundant readily available large models, such as ChatGPT, LLaMa, and T5. It then becomes natural to ask&#58; Can we distill the knowledge from multiple teachers? At first glance, it appears easy to perform multi-teacher KD, as we can simply train the student from the union of teachers’ predictions. However, I would argue that such a naïve attempt may not work well for multi-teacher KD. This is because traditional KD adopts the cross-entropy loss, which tends to yield a smooth distribution. In this talk, I will present a novel ensemble-then-distill approach, which builds an ensemble of teacher models to train the student. I will also discuss applications to text generation and syntactic parsing.  

- Title: Morning Break
  Time: 9:30AM - 10:00AM
  Whole_line: true  


- Title: (<b>KeyNote Talk</b>) Hardware-aware Algorithms for Language Modeling
  Time: 10:00AM - 10:30AM
  Presenter: Prof. Tri Dao
  Is_paper: false
  Whole_line: false
  Bio: <b>Tri Dao</b> is an Assistant Professor at Princeton University and chief scientist of Together AI. He completed his PhD in Computer Science at Stanford. He works at the intersection of machine learning and systems, and his research interests include sequence models with long-range memory and structured matrices for compact deep learning models. His work has received the COLM 2024 Outstanding paper award and ICML 2022 Outstanding paper runner-up award.
  Abstract: Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. We describe recent progress on subquadratic-time architectures such structured state space models (SSMs). We identify that a key weakness of such models is their inability to perform content-based reasoning, and propose a selection mechanism to address this shortcoming. Though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks. The resulting architecture (Mamba and Mamba-2) matches or exceeds the performance of strong modern Transformers on language modeling, validated at 3B scales on both pretraining and downstream evaluation, while enjoying 5x higher inference throughput and linear scaling in sequence length. Hybridizing Mamba layers with a 2-4 attention layers leads to state-of-the-art models, excelling at long context and fast inference.
- Title: (<b>KeyNote Talk</b>) Speech generative modeling with little tokenization
  Time: 10:30AM - 11:00AM
  Presenter: Dr. Navdeep Jaitly
  Is_paper: false
  Whole_line: false
  Bio: <b>Navdeep Jaitly</b> is a Research Scientist at Apple Machine Learning Research (MLR) where he leads a team of researchers working on fundamental techniques in Machine Learning with an emphasis on speech and language. He got his PhD from University of Toronto, under the supervision of Geoffrey Hinton in the foundational days of Deep Learning. During a PhD internship at Google in 2011 he demonstrated how Deep Neural Networks would revolutionize speech recognition replacing HMM systems that were in use before. After his PhD he joined Google Brain working on sequence models and techniques such as Listen Attend and Spell, Adversarial Autoencoders and Pointer Networks. He has also held machine learning research positions at Nvidia, Google Brain Robotics (initiating robotic ping pong), D. E. Shaw and the National Labs.
  Abstract: It is well accepted now that speech needs to be tokenized before it can be modeled with transformer based generative models. In fact there is a rich body of intricate work using semantic and other acoustic tokens for speech modeling. In this talk we show how tokenization may not be necessary and that, indeed, a simple way of discretizing Mel-spectrograms (which we call d-Mel) is enough to build generative models with transformers. We show how we can build conditional generative models of speech (text-to-speech) using d-Mel and transformer based models. We also demonstrate that the same technique can be applied to multi-modal generation of speech conditioned on text and video. It is our hope that this leads to more exploration on minimal preprocessing of speech for use in generative modeling.
- Title: (<b>KeyNote Talk</b>) Title TBD
  Time: 11:00AM - 11:30AM
  Presenter: Prof. Danqi Chen
  Is_paper: false
  Whole_line: false
  Bio: <b>Danqi Chen</b> co-leads Princeton's NLP Group, researches large language models, and emphasizes practicality and accessibility in AI development.
  Abstract: Abstract TBD
- Title: (<b>Spotlight 1</b>) Sparsified State-Space Models are Efficient Highway Networks
  Time: 11:30AM - 11:36AM
  Presenter: Woomin Song
  Is_paper: true
  Whole_line: false
  Authors: Woomin Song (KAIST),Jihoon Tack (KAIST),Sangwoo Mo (University of Michigan),Seunghyuk Oh (KAIST),Jinwoo Shin (KAIST)
  Abstract: State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences.
- Title: (<b>Spotlight 2</b>) Longhorn&#58; State Space Models are Amortized Online Learners
  Time: 9:36AM - 9:42AM
  Presenter: Bo Liu
  Is_paper: true
  Whole_line: false
  Authors: Bo Liu (University of Texas, Austin), Rui Wang (Helixon),Lemeng Wu (University of Texas, Austin),Yihao Feng (University of Texas, Austin ),Peter Stone (University of Texas at Austin and Sony AI),Qiang Liu (UT Austin)
  Abstract: Modern large language models are built on sequence modeling via next-token prediction.<br/>While the Transformer remains the dominant architecture for sequence modeling, its quadratic decoding complexity in sequence length poses a major limitation. State-space models (SSMs) present a competitive alternative, offering linear decoding efficiency while maintaining parallelism during training. However, most existing SSMs rely on linear recurrence designs that appear somewhat ad hoc.<br/>In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from solving these objectives.<br/>Based on this insight, we introduce a novel deep SSM architecture, Longhorn, whose update resembles the closed-form solution for solving the online associative recall problem. Our experimental results show that Longhorn outperforms state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks, language modeling, and vision tasks. Specifically, Longhorn achieves a 1.8x improvement in sample efficiency compared to Mamba, and can extrapolate over contexts that are up to 16x longer during inference. 
- Title: (<b>Spotlight 3</b>) Title TBD
  Time: 9:42AM - 9:48AM
  Presenter: Yu Yang
  Is_paper: true
  Whole_line: false
  Authors: Yu Yang · Aaditya Singh · Mostafa Elhoushi · Anas Mahmoud · Kushal Tirumala · Fabian Gloeckle · Baptiste Roziere · Carole-Jean Wu · Ari Morcos · Newsha Ardalani
  Abstract: Abstract TBD
- Title: (<b>Spotlight 4</b>) Title TBD
  Time: 9:48AM - 9:54AM
  Presenter: Dan Fu
  Is_paper: true
  Whole_line: false
  Authors: Dan Fu · Hermann Kumbong · Eric Nguyen · Christopher Ré
  Abstract: Abstract TBD
- Title: (<b>Spotlight 5</b>) Title TBD
  Time: 9:54AM - 10:00AM
  Presenter: Xi Wang
  Is_paper: true
  Whole_line: false
  Authors: Xi Wang · Laurence Aitchison · Maja Rudolph
  Abstract: Abstract TBD





  

- Title: Accepted Oral Presentations
  Time: 12:00PM - 12:30PM
  Presenter: TBD
  Is_paper: true
  Whole_line: false
  Authors: TBD
  Abstract: Abstract TBD
# - Title: (<b>Spotlight 7</b>) Title TBD
#   Time: 11:36AM - 11:42AM
#   Presenter: Dong-Ki Kim
#   Is_paper: true
#   Whole_line: false
#   Authors: Dong-Ki Kim · Sungryull Sohn · Lajanugen Logeswaran · Dongsub Shim · Honglak Lee
#   Abstract: Abstract TBD
# - Title: (<b>Spotlight 8</b>) Title TBD
#   Time: 11:42AM - 11:48PM
#   Presenter: Yixiao Li
#   Is_paper: true
#   Whole_line: false
#   Authors: Yixiao Li · Yifan Yu · Chen Liang · Nikos Karampatziakis · Pengcheng He · Weizhu Chen · Tuo Zhao
#   Abstract: Abstract TBD
# - Title: (<b>Spotlight 9</b>) Title TBD
#   Time: 11:48AM - 11:54PM
#   Presenter: Michael Zhang
#   Is_paper: true
#   Whole_line: false
#   Authors: Michael Zhang · Kush Bhatia · Hermann Kumbong · Christopher Ré
#   Abstract: Abstract TBD
# - Title: (<b>Spotlight 10</b>) Title TBD
#   Time: 11:54AM - 12:00PM
#   Presenter: Giovanni Monea
#   Is_paper: true
#   Whole_line: false
#   Authors: Giovanni Monea · Armand Joulin · Edouard Grave
#   Abstract: Abstract TBD

- Title: Lunch Break
  Time: 12:30PM - 1:15PM
  Whole_line: true

 
- Title: <b>Poster Session I </b> & Free Discussion
  Time: 1:15PM - 2:00PM
  Whole_line: true

- Title: (<b>KeyNote Talk</b>)  The LoRA Journey and Learnings&#58 from Creation to Industrial-Scale Adoption
  Time: 2:00PM - 2:30PM
  Presenter: Dr. Weizhu Chen
  Is_paper: false
  Whole_line: false
  Bio: <b>Weizhu Chen</b> Weizhu Chen is the Vice President leading the Microsoft GenAI modeling team, driving innovation in large-scale AI model training, including pre-training, post-training, and evaluation for both Microsoft and OpenAI. Under his leadership, the team has pioneered groundbreaking advancements such as LoRA, DeBERTa, and Phi-3 models. With over 19 years at Microsoft, Weizhu has held pivotal roles in shaping AI and machine learning technologies. Previously, he served as Partner Science Manager at Microsoft Azure AI and led teams in the Business Applications Group and Research divisions, focusing on deep learning, NLP, and distributed machine learning at cloud scale. Before joining Microsoft, he contributed to research on information retrieval at IBM Research. Weizhu’s career reflects a deep commitment to advancing the state of AI, making a significant impact on the field and enabling transformative technologies.
  Abstract: Abstract TBD
  
- Title: (<b>KeyNote Talk</b>) Title TBD
  Time: 2:30PM - 3:00PM
  Presenter: Prof. Hananeh Hajishirzi
  Is_paper: false
  Whole_line: false
  Bio: <b>Hananeh Hajishirzi</b>, a leading NLP expert focusing on large language models, explores how AI can reason and understand complex information from various sources.
  Abstract: Abstract TBD



- Title: Afternoon Break
  Time: 03:00PM - 03:15PM
  Whole_line: true

- Title: <b>Interactive Panel Discussion</b>
  Time: 3:20PM - 4:10PM
  Presenter: <ul><li> Marjan Ghazvini Nejad</li><li> Joel Hestness</li><li> Katie Derthick</li><li> Lu Hou</li></ul>
  Is_paper: false
  Whole_line: false
  
- Title: Best Paper and Poster Awards 
  Time: 4:10PM-4:15PM
  Whole_line: true  
  
- Title: <b>Poster Session II </b> & Free Discussion
  Time: 4:15PM - 5:00PM
  Whole_line: true



