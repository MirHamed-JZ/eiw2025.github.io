# - Title: Breakfast 
#   Time: 8:10AM - 8:20AM
#   Whole_line: true
# 
- Title: Opening Speech
  Time: 8:10AM - 8:15AM
  Whole_line: true  

- Title: (<b>KeyNote Talk</b>) Multi-Teacher Distillation&#58; An Ensemble-Then-Distill Approach
  Time: 8:15AM - 8:45AM
  Presenter: Prof. Lili Mou
  Is_paper: false
  Whole_line: false
  Bio: <b> Dr. Lili Mou</b> is an Assistant Professor at the Department of Computing Science, University of Alberta. He is also an Alberta Machine Intelligence Institute (Amii) Fellow and a Canada CIFAR AI (CCAI) Chair. Lili received his BS and PhD degrees in 2012 and 2017, respectively, from School of EECS, Peking University. After that, he worked as a postdoctoral fellow at the University of Waterloo. His research interests mainly lie in designing novel machine learning algorithms and frameworks for NLP. He has publications at top conferences and journals, including ACL, EMNLP, TACL, ICML, ICLR, and NeurIPS. He also presented tutorials at EMNLP'19 and ACL'20. He received a AAAI New Faculty Highlight Award in 2021.
  Abstract: Knowledge distillation (KD) aims to transfer the knowledge in a large model (called a teacher) into a small one (called a student), and has become an emerging research topic as the sizes of deep learning models keep growing. Today, there are abundant readily available large models, such as ChatGPT, LLaMa, and T5. It then becomes natural to ask&#58; Can we distill the knowledge from multiple teachers? At first glance, it appears easy to perform multi-teacher KD, as we can simply train the student from the union of teachers’ predictions. However, I would argue that such a naïve attempt may not work well for multi-teacher KD. This is because traditional KD adopts the cross-entropy loss, which tends to yield a smooth distribution. In this talk, I will present a novel ensemble-then-distill approach, which builds an ensemble of teacher models to train the student. I will also discuss applications to text generation and syntactic parsing.  

- Title: (<b>KeyNote Talk</b>) Efficiency through Learning from Experience
  Time: 8:45AM - 9:15AM
  Presenter: Dr. Bhavana Dalvi Mishra
  Is_paper: false
  Whole_line: false
  Bio: <b>Dr. Bhavana Dalvi Mishra</b> is a Lead Research Scientist at the Allen Institute for AI (Ai2). Her research focuses on NLP, interactive reasoning, and scientific discovery. She obtained her Ph.D. in Computer Science from Carnegie Mellon University in 2015 and earned her Master's in Computer Science from the Indian Institute of Technology, Bombay in 2007. She has received several awards, including two Best Paper runner-up awards, Google Ph.D. fellowship, and Barbara Lazarus Women@IT Fellowship from CMU for her contributions to NLP and AI.
  Abstract: Despite the physiological limitations of the human brain, humans are remarkably efficient thinkers, in large part because they can learn from experience, allowing them to avoid prior reasoning errors and quickly jump to conclusions that previously took substantial effort. Similarly, language models (LMs) can rapidly improve their inference-time efficiency through inference-time learning, supplementing lower-level methods like fast decoding and caching. I'll describe two agent-based systems (CLIN and SSO) that do this, using an external RAG (retrieval-augmented generation) memory to help the agent navigate a complex, virtual environment. Unlike typical RAG systems, the memory is dynamic and updated after each task (including forgetting unhelpful learnings). In addition, unlike reinforcement-based continual learning techniques, these systems rapidly learn from just a handful of examples by exploiting LMs to conjecture useful generalizations of past experiences.  I'll outline three critical activities in this process - what to remember, how to index those memories, and how to retrieve from that index - and how those choices impact the effectiveness of the resulting agent. While this concept of efficiency is a little different to foundational architectural considerations, I'll show that it is nonetheless powerful, and an important additional tool in the toolbox for efficient future applications.
  

- Title: Accepted Oral Presentations 
  Time: 9:15AM - 10:00AM
  Presenter: TBD
  Is_paper: true
  Whole_line: false
  Authors: TBD
  Abstract: Abstract TBD
# - Title: (<b>Spotlight 2</b>) Title TBD
#   Time: 9:36AM - 9:42AM
#   Presenter: Fnu Devvrit
#   Is_paper: true
#   Whole_line: false
#   Authors: Fnu Devvrit · Sneha Kudugunta · Aditya Kusupati · Tim Dettmers · Kaifeng Chen · Inderjit Dhillon · Yulia Tsvetkov · Hanna Hajishirzi · Sham Kakade · Ali Farhadi · Prateek Jain
#   Abstract: Abstract TBD
# - Title: (<b>Spotlight 3</b>) Title TBD
#   Time: 9:42AM - 9:48AM
#   Presenter: Yu Yang
#   Is_paper: true
#   Whole_line: false
#   Authors: Yu Yang · Aaditya Singh · Mostafa Elhoushi · Anas Mahmoud · Kushal Tirumala · Fabian Gloeckle · Baptiste Roziere · Carole-Jean Wu · Ari Morcos · Newsha Ardalani
#   Abstract: Abstract TBD
# - Title: (<b>Spotlight 4</b>) Title TBD
#   Time: 9:48AM - 9:54AM
#   Presenter: Dan Fu
#   Is_paper: true
#   Whole_line: false
#   Authors: Dan Fu · Hermann Kumbong · Eric Nguyen · Christopher Ré
#   Abstract: Abstract TBD
# - Title: (<b>Spotlight 5</b>) Title TBD
#   Time: 9:54AM - 10:00AM
#   Presenter: Xi Wang
#   Is_paper: true
#   Whole_line: false
#   Authors: Xi Wang · Laurence Aitchison · Maja Rudolph
#   Abstract: Abstract TBD



- Title: Morning Break
  Time: 10:00AM - 10:30AM
  Whole_line: true


- Title: (<b>KeyNote Talk</b>) Title TBD
  Time: 10:30AM - 11:00AM
  Presenter: Prof. Tri Dao
  Is_paper: false
  Whole_line: false
  Bio: <b>Tri Dao</b> Tri Dao is an Assistant Professor at Princeton University and chief scientist of Together AI. He completed his PhD in Computer Science at Stanford. He works at the intersection of machine learning and systems, and his research interests include sequence models with long-range memory and structured matrices for compact deep learning models. His work has received the COLM 2024 Outstanding paper award and ICML 2022 Outstanding paper runner-up award.
  Abstract: Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. We describe recent progress on subquadratic-time architectures such structured state space models (SSMs). We identify that a key weakness of such models is their inability to perform content-based reasoning, and propose a selection mechanism to address this shortcoming. Though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks. The resulting architecture (Mamba and Mamba-2) matches or exceeds the performance of strong modern Transformers on language modeling, validated at 3B scales on both pretraining and downstream evaluation, while enjoying 5x higher inference throughput and linear scaling in sequence length. Hybridizing Mamba layers with a 2-4 attention layers leads to state-of-the-art models, excelling at long context and fast inference.
- Title: (<b>KeyNote Talk</b>) Hardware-aware Algorithms for Language Modeling
  Time: 11:00AM - 11:30AM
  Presenter: Dr. Navdeep Jaitly
  Is_paper: false
  Whole_line: false
  Bio: <b>Navdeep Jaitly</b> worked under Geoffrey Hinton at the University of Toronto, his interest lie in pushing the frontier of Deep Learning research deep learning for Apple, following work on Google's Brain team.
  Abstract: Abstract TBD
- Title: (<b>KeyNote Talk</b>) Title TBD
  Time: 11:30AM - 12:00AM
  Presenter: Prof. Danqi Chen
  Is_paper: false
  Whole_line: false
  Bio: <b>Danqi Chen</b> co-leads Princeton's NLP Group, researches large language models, and emphasizes practicality and accessibility in AI development.
  Abstract: Abstract TBD
  

- Title: Accepted Oral Presentations
  Time: 12:00PM - 12:30PM
  Presenter: TBD
  Is_paper: true
  Whole_line: false
  Authors: TBD
  Abstract: Abstract TBD
# - Title: (<b>Spotlight 7</b>) Title TBD
#   Time: 11:36AM - 11:42AM
#   Presenter: Dong-Ki Kim
#   Is_paper: true
#   Whole_line: false
#   Authors: Dong-Ki Kim · Sungryull Sohn · Lajanugen Logeswaran · Dongsub Shim · Honglak Lee
#   Abstract: Abstract TBD
# - Title: (<b>Spotlight 8</b>) Title TBD
#   Time: 11:42AM - 11:48PM
#   Presenter: Yixiao Li
#   Is_paper: true
#   Whole_line: false
#   Authors: Yixiao Li · Yifan Yu · Chen Liang · Nikos Karampatziakis · Pengcheng He · Weizhu Chen · Tuo Zhao
#   Abstract: Abstract TBD
# - Title: (<b>Spotlight 9</b>) Title TBD
#   Time: 11:48AM - 11:54PM
#   Presenter: Michael Zhang
#   Is_paper: true
#   Whole_line: false
#   Authors: Michael Zhang · Kush Bhatia · Hermann Kumbong · Christopher Ré
#   Abstract: Abstract TBD
# - Title: (<b>Spotlight 10</b>) Title TBD
#   Time: 11:54AM - 12:00PM
#   Presenter: Giovanni Monea
#   Is_paper: true
#   Whole_line: false
#   Authors: Giovanni Monea · Armand Joulin · Edouard Grave
#   Abstract: Abstract TBD

- Title: Lunch Break
  Time: 12:30PM - 1:15PM
  Whole_line: true

 
- Title: <b>Poster Session I </b> & Free Discussion
  Time: 1:15PM - 2:00PM
  Whole_line: true

- Title: (<b>KeyNote Talk</b>)  The LoRA Journey and Learnings&#58 from Creation to Industrial-Scale Adoption
  Time: 2:00PM - 2:30PM
  Presenter: Dr. Weizhu Chen
  Is_paper: false
  Whole_line: false
  Bio: <b>Weizhu Chen</b> Weizhu Chen is the Vice President leading the Microsoft GenAI modeling team, driving innovation in large-scale AI model training, including pre-training, post-training, and evaluation for both Microsoft and OpenAI. Under his leadership, the team has pioneered groundbreaking advancements such as LoRA, DeBERTa, and Phi-3 models. With over 19 years at Microsoft, Weizhu has held pivotal roles in shaping AI and machine learning technologies. Previously, he served as Partner Science Manager at Microsoft Azure AI and led teams in the Business Applications Group and Research divisions, focusing on deep learning, NLP, and distributed machine learning at cloud scale. Before joining Microsoft, he contributed to research on information retrieval at IBM Research. Weizhu’s career reflects a deep commitment to advancing the state of AI, making a significant impact on the field and enabling transformative technologies.
  Abstract: Abstract TBD
  
- Title: (<b>KeyNote Talk</b>) Title TBD
  Time: 2:30PM - 3:00PM
  Presenter: Prof. Hananeh Hajishirzi
  Is_paper: false
  Whole_line: false
  Bio: <b>Hananeh Hajishirzi</b>, a leading NLP expert focusing on large language models, explores how AI can reason and understand complex information from various sources.
  Abstract: Abstract TBD



- Title: Afternoon Break
  Time: 03:00PM - 03:15PM
  Whole_line: true

- Title: <b>Interactive Panel Discussion</b>
  Time: 3:20PM - 4:10PM
  Presenter: <ul><li> Marjan Ghazvini Nejad</li><li> Joel Hestness</li><li> Katie Derthick</li><li> Lu Hou</li></ul>
  Is_paper: false
  Whole_line: false
  
- Title: Best Paper and Poster Awards 
  Time: 4:10PM-4:15PM
  Whole_line: true  
  
- Title: <b>Poster Session II </b> & Free Discussion
  Time: 4:15PM - 5:00PM
  Whole_line: true



